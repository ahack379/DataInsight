{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.rc('font',family='serif')\n",
    "matplotlib.rc('font',weight='bold')\n",
    "matplotlib.rc('font',size=16)\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "from recommender_functions import *\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_curve,roc_auc_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = 'Week4'\n",
    "day  = 'Mon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the ratings by user ID: starts at user index 1 \n",
    "r = pd.read_csv('goodbooks-10k/ratings.csv')\n",
    "df_ratings = pd.DataFrame(r)\n",
    "\n",
    "# Add counts to the df\n",
    "cut_u, cut_b = 175, 1 #175, 1\n",
    "df_ratings['user_counts'] = df_ratings.groupby(['user_id'])['book_id'].transform('count')\n",
    "df_ratings['book_counts'] = df_ratings.groupby(['book_id'])['user_id'].transform('count')\n",
    "df_ratings_cut = df_ratings.query('user_counts > %d '%(cut_u))\n",
    "\n",
    "# Add an index for the user for matrix making later\n",
    "df_ratings_cut['user_idx'] = pd.Categorical(df_ratings_cut['user_id']).codes \n",
    "df_ratings_cut['book_idx'] = pd.Categorical(df_ratings_cut['book_id']).codes \n",
    "\n",
    "df_ratings_cut.to_csv('ratings_cut.csv', sep=',')\n",
    "\n",
    "# ...but also do a groupby, so can plot easily\n",
    "df_ratings_count_u = df_ratings.groupby(['user_id']).size().reset_index(name='Counts')\n",
    "\n",
    "r = pd.read_csv('goodbooks-10k/books_with_genres.csv')\n",
    "df_books = pd.DataFrame(r)\n",
    "df_ratings_cut.head()\n",
    "\n",
    "r = pd.read_csv('goodbooks-10k/ahack_tags_3.csv')\n",
    "df_tags = pd.DataFrame(r)\n",
    "df_tags['tag_index'] = pd.Categorical(df_tags['tag_id']).codes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of books and users variables for later use\n",
    "N_BOOKS = len(df_ratings_cut.book_id.unique()) \n",
    "N_USERS = len(df_ratings_cut.user_id.unique())\n",
    "\n",
    "print(N_BOOKS,N_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Rank Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, define a matrix size num_books x num_users\n",
    "ratings_genre_mat = np.zeros((N_USERS,N_BOOKS))\n",
    "ratings_mat = np.zeros((N_USERS,N_BOOKS))\n",
    "binary_mat = np.zeros((N_USERS,N_BOOKS))\n",
    "\n",
    "Y, R = ratings_mat, binary_mat\n",
    "\n",
    "genre_list_dict = []\n",
    "\n",
    "# Now fill the rank matrix and validation matrix\n",
    "for i in range(df_ratings_cut.shape[0]):\n",
    "    user_i = df_ratings_cut.user_idx.values[i] # This goes from 0 -> 536\n",
    "    book_i = df_ratings_cut.book_idx.values[i] # This goes from 0 -> 7336\n",
    "    rating_i = df_ratings_cut.rating.values[i] # This goes from 1 -> 5\n",
    "\n",
    "    # Fill ratings + binary matrix\n",
    "    ratings_mat[user_i][book_i] = rating_i\n",
    "    binary_mat[user_i][book_i] = 1\n",
    "    \n",
    "    # Now fill the genre tag matrix\n",
    "    # First link ratings matrix entry to its book in df_books\n",
    "    actual_book_i = df_ratings_cut.book_id.values[i]               # book_id goes from 1 -> 10000\n",
    "    book_q = df_books.loc[df_books['book_id'] == actual_book_i]\n",
    "    genre, tag_id = book_q.genre.values[0], book_q.tag_id.values[0] \n",
    "    \n",
    "    ratings_genre_mat[user_i][book_i] = int(tag_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ave_v = ratings_mat.sum(1)/(ratings_mat!=0).sum(1).astype(float)\n",
    "book_ave_v = ratings_mat.T.sum(1)/(ratings_mat.T!=0).sum(1).astype(float)\n",
    "\n",
    "ave_mat = np.zeros((N_USERS,N_BOOKS))\n",
    "user_ave_mat = np.zeros((N_USERS,N_BOOKS))\n",
    "\n",
    "for i in range(N_USERS):\n",
    "    rowi = [ (user_ave_v[i]+book_ave_v[j])/2 for j in range(N_BOOKS) ]\n",
    "    ave_mat[i] = rowi\n",
    "    \n",
    "    row_ave = [user_ave_v[i] for j in range(N_BOOKS)]\n",
    "    user_ave_mat[i] = row_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per empty entry, set entry value to average of user and book rankings\n",
    "orig_ratings_mat = ratings_mat.copy()\n",
    "temp = (ratings_mat == 0)\n",
    "ratings_mat[temp] = ave_mat[temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now store the top 3 genre preferences per user \n",
    "top_3_genres_per_user = np.zeros((N_USERS,4))\n",
    "top_3_genres_names_per_user = []\n",
    "top_genre_names_per_user = []\n",
    "\n",
    "for rowi in range(ratings_genre_mat.shape[0]):\n",
    "    \n",
    "    i = ratings_genre_mat[rowi]\n",
    "    \n",
    "    unique, counts = np.unique(i,return_counts=True)\n",
    "\n",
    "    pred_idxs_sorted = np.argsort(counts)\n",
    "    pred_idxs_sorted = pred_idxs_sorted[::-1]\n",
    "\n",
    "    # Start at 1, not 0, to avoid the 0's which are most of the space\n",
    "    top_3_genres_per_user[rowi][0] = unique[pred_idxs_sorted[1]] #counts[pred_idx_sorted[1]]\n",
    "    top_3_genres_per_user[rowi][1] = unique[pred_idxs_sorted[2]] #counts[pred_idx_sorted[1]]\n",
    "    top_3_genres_per_user[rowi][2] = unique[pred_idxs_sorted[3]] #counts[pred_idx_sorted[1]]\n",
    "    top_3_genres_per_user[rowi][3] = unique[pred_idxs_sorted[4]] #counts[pred_idx_sorted[1]]\n",
    "    \n",
    "    first_pick = df_tags.query('tag_id == %d'%int(top_3_genres_per_user[rowi][0]))\n",
    "    second_pick = df_tags.query('tag_id == %d'%int(top_3_genres_per_user[rowi][1]))\n",
    "    third_pick = df_tags.query('tag_id == %d'%int(top_3_genres_per_user[rowi][2]))\n",
    "\n",
    "    top_genre_names_per_user.append(first_pick['tag_id'].values[0])\n",
    "    \n",
    "    user_pref_v = [first_pick['tag_name'].values[0], second_pick['tag_name'].values[0], third_pick['tag_name'].values[0]]\n",
    "\n",
    "    top_3_genres_names_per_user.append(user_pref_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok cool, so now per user, build a matrix to blind user's top genres\n",
    "# This matrix is the same size as the ratings matrix, but only has entries for \n",
    "# preferences below the user's favorite\n",
    "binary_genre_mat = binary_mat.copy()\n",
    "\n",
    "for i in range(ratings_genre_mat.shape[0]):\n",
    "    rowi = ratings_genre_mat[i]\n",
    "    binary_genre_mat[i] = [ 1 if ratings_genre_mat[i][j] != 0 and ratings_genre_mat[i][j] != top_genre_names_per_user[i]\\\n",
    "            else 0 for j in range(ratings_genre_mat.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data between train and test, user similarity (roughly 80:20 split)\n",
    "train_set, test_set = train_test_split(ratings_mat,split=1400)\n",
    "user_similarity = fast_similarity(train_set,kind='user')\n",
    "\n",
    "print('Test :' ,float(len(np.nonzero(test_set)[0]))/(len(np.nonzero(train_set)[0])+len(np.nonzero(test_set)[0]))*100)\n",
    "print('Train:' ,float(len(np.nonzero(train_set)[0]))/(len(np.nonzero(train_set)[0])+len(np.nonzero(test_set)[0]))*100)\n",
    "\n",
    "print(len(np.nonzero(test_set)[0]),len(np.nonzero(train_set)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user predictions for only top k = 30 (mse minimum) most similar users \n",
    "user_prediction_topk = predict_topk(train_set, user_similarity, kind='user', k=30)\n",
    "print( 'Top-k User-based CF MSE: ' + str(get_mse(user_prediction_topk, test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also find the item similarity\n",
    "# This does not perform as well\n",
    "#item_similarity = fast_similarity(train_set,kind='item')\n",
    "#item_prediction_topk = predict_topk(train_set,item_similarity,kind='item',k=15)\n",
    "\n",
    "#item_y_pred_topk = item_prediction_topk[nonzero_test]\n",
    "#item_y_pred_scaled_topk = (item_y_pred_topk - 1.) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blind data from user's top genre \n",
    "blind_test_set = test_set.copy() * binary_genre_mat\n",
    "nonzero_test = blind_test_set > 0\n",
    "\n",
    "blind_y_true = blind_test_set[nonzero_test]\n",
    "blind_y_pred_topk = user_prediction_topk[nonzero_test]\n",
    "user_ave_thresholds = user_ave_mat[nonzero_test]\n",
    "\n",
    "blind_y_pred_scaled_topk = (blind_y_pred_topk - 1.) / 4\n",
    "\n",
    "# Binarize true values and predictions using user's average rating as a threshold\n",
    "blind_y_true_binarized = binarize(blind_y_true.copy(), user_ave_thresholds)\n",
    "blind_y_pred_binarized_topk = binarize(blind_y_pred_topk.copy(), user_ave_thresholds) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = np.tile(np.array([np.arange(blind_test_set.shape[0])]).T, \\\n",
    "                   (1, blind_test_set.shape[1]))[nonzero_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(true, pred, pred_binarized, user_ids, k, tol=[]):\n",
    "    unique_users = np.unique(user_ids)\n",
    "    precisions = np.zeros(unique_users.size)\n",
    "    \n",
    "    for i in range(unique_users.size):\n",
    "        user_ind = user_ids == unique_users[i]\n",
    "        user_true = true[user_ind]\n",
    "        user_pred = pred[user_ind]\n",
    "        user_pred_binarized = pred_binarized[user_ind]\n",
    "        ranked_ind = np.argsort(-user_pred)[:k]\n",
    "        precisions[i] = precision_score(user_true[ranked_ind], user_pred_binarized[ranked_ind])\n",
    "    return np.mean(precisions[precisions > 0]) #precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = user_ave_thresholds\n",
    "print( 'precision @4 : ', precision_at_k(blind_y_true_binarized, blind_y_pred_topk, blind_y_pred_binarized_topk, user_ids, 4, tol=t))\n",
    "print( 'precision @8 : ', precision_at_k(blind_y_true_binarized, blind_y_pred_topk, blind_y_pred_binarized_topk, user_ids, 8, tol=t))\n",
    "print( 'precision @16 : ', precision_at_k(blind_y_true_binarized, blind_y_pred_topk, blind_y_pred_binarized_topk, user_ids, 16, tol=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_v = []\n",
    "t = user_ave_thresholds\n",
    "\n",
    "for i in range(1,30,1):\n",
    "\n",
    "    p =  precision_at_k(blind_y_true_binarized, blind_y_pred_topk, blind_y_pred_binarized_topk, user_ids, i, tol=t)\n",
    "    precision_v.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "x=np.linspace(1,31,29)\n",
    "plt.plot(x,precision_v,'-o',color='green')\n",
    "plt.xlabel('# of Recommendations',fontsize=18)\n",
    "plt.ylabel('Precision',fontsize=18)\n",
    "plt.grid(True,ls='--')\n",
    "plt.savefig('Plots/%s_%s_precision.png'%(week,day), bbox_inches='tight')\n",
    "plt.xlim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall, precision, and f1 are definied for binarized predictions  \n",
    "auc_score = roc_auc_score(blind_y_true_binarized, blind_y_pred_scaled_topk)\n",
    "fpr_b, tpr_b, thresholds = roc_curve(blind_y_true_binarized, blind_y_pred_scaled_topk)\n",
    "\n",
    "print( 'Recall: %0.3f' % recall_score(blind_y_true_binarized, blind_y_pred_binarized_topk))\n",
    "print( 'Precision: %0.3f' % precision_score(blind_y_true_binarized, blind_y_pred_binarized_topk))\n",
    "print( 'F1 score: %0.3f' % f1_score(blind_y_true_binarized, blind_y_pred_binarized_topk))\n",
    "print( 'ROC-AUC: %0.3f' % auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blind_y_true_binarized)\n",
    "print(blind_y_pred_scaled_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_b, tpr_b, lw=2,color='green',label='User-Similarity') \n",
    "plt.plot([0, 1], [0, 1], 'k--',lw=2,label='Random')\n",
    "\n",
    "plt.xlabel('False Positive Rate',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.legend(fontsize=16)\n",
    "plt.savefig('Plots/%s_%s_roc.png'%(week,day),bbox_to_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
